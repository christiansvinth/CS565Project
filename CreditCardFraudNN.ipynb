{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satellite-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "automatic-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "danish-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionData = cc_data.drop(['Time'], axis=1)\n",
    "transactionData['Amount'] = StandardScaler().fit_transform(transactionData['Amount'].values.reshape(-1, 1))\n",
    "assert(len(X) == len(y))\n",
    "\n",
    "X = transactionData.drop(\"Class\", axis=1).values\n",
    "y = transactionData['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "athletic-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.as_tensor(X)\n",
    "y_tensor = torch.as_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "experimental-style",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([284807, 29])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "occupational-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(29, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "embedded-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binaryClassification().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "waiting-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionData = cc_data.drop(['Time'], axis=1)\n",
    "transactionData['Amount'] = StandardScaler().fit_transform(transactionData['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "controversial-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56962\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "alike-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "minibatch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "integral-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=1)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "\n",
    "test_data = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=minibatch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eight-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "affecting-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "graduate-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "history['train_loss'] = []\n",
    "history['test_loss'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "hollywood-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "boring-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.01705 | Acc: 99.688\n",
      "Epoch 002: | Loss: 0.00365 | Acc: 99.926\n",
      "Epoch 003: | Loss: 0.00323 | Acc: 99.930\n",
      "Epoch 004: | Loss: 0.00292 | Acc: 99.938\n",
      "Epoch 005: | Loss: 0.00284 | Acc: 99.938\n",
      "Epoch 006: | Loss: 0.00261 | Acc: 99.939\n",
      "Epoch 007: | Loss: 0.00257 | Acc: 99.938\n",
      "Epoch 008: | Loss: 0.00243 | Acc: 99.941\n",
      "Epoch 009: | Loss: 0.00225 | Acc: 99.944\n",
      "Epoch 010: | Loss: 0.00221 | Acc: 99.945\n",
      "Epoch 011: | Loss: 0.00203 | Acc: 99.948\n",
      "Epoch 012: | Loss: 0.00203 | Acc: 99.950\n",
      "Epoch 013: | Loss: 0.00203 | Acc: 99.951\n",
      "Epoch 014: | Loss: 0.00180 | Acc: 99.950\n",
      "Epoch 015: | Loss: 0.00183 | Acc: 99.953\n",
      "Epoch 016: | Loss: 0.00175 | Acc: 99.960\n",
      "Epoch 017: | Loss: 0.00178 | Acc: 99.955\n",
      "Epoch 018: | Loss: 0.00184 | Acc: 99.956\n",
      "Epoch 019: | Loss: 0.00165 | Acc: 99.957\n",
      "Epoch 020: | Loss: 0.00158 | Acc: 99.961\n",
      "Epoch 021: | Loss: 0.00145 | Acc: 99.961\n",
      "Epoch 022: | Loss: 0.00160 | Acc: 99.954\n",
      "Epoch 023: | Loss: 0.00143 | Acc: 99.963\n",
      "Epoch 024: | Loss: 0.00138 | Acc: 99.961\n",
      "Epoch 025: | Loss: 0.00136 | Acc: 99.965\n",
      "Epoch 026: | Loss: 0.00118 | Acc: 99.968\n",
      "Epoch 027: | Loss: 0.00132 | Acc: 99.965\n",
      "Epoch 028: | Loss: 0.00131 | Acc: 99.963\n",
      "Epoch 029: | Loss: 0.00135 | Acc: 99.966\n",
      "Epoch 030: | Loss: 0.00124 | Acc: 99.966\n",
      "Epoch 031: | Loss: 0.00114 | Acc: 99.971\n",
      "Epoch 032: | Loss: 0.00124 | Acc: 99.964\n",
      "Epoch 033: | Loss: 0.00099 | Acc: 99.973\n",
      "Epoch 034: | Loss: 0.00129 | Acc: 99.966\n",
      "Epoch 035: | Loss: 0.00113 | Acc: 99.967\n",
      "Epoch 036: | Loss: 0.00122 | Acc: 99.965\n",
      "Epoch 037: | Loss: 0.00106 | Acc: 99.970\n",
      "Epoch 038: | Loss: 0.00107 | Acc: 99.970\n",
      "Epoch 039: | Loss: 0.00109 | Acc: 99.968\n",
      "Epoch 040: | Loss: 0.00113 | Acc: 99.968\n",
      "Epoch 041: | Loss: 0.00112 | Acc: 99.967\n",
      "Epoch 042: | Loss: 0.00096 | Acc: 99.970\n",
      "Epoch 043: | Loss: 0.00102 | Acc: 99.970\n",
      "Epoch 044: | Loss: 0.00094 | Acc: 99.971\n",
      "Epoch 045: | Loss: 0.00104 | Acc: 99.972\n",
      "Epoch 046: | Loss: 0.00095 | Acc: 99.971\n",
      "Epoch 047: | Loss: 0.00088 | Acc: 99.971\n",
      "Epoch 048: | Loss: 0.00086 | Acc: 99.975\n",
      "Epoch 049: | Loss: 0.00093 | Acc: 99.969\n",
      "Epoch 050: | Loss: 0.00088 | Acc: 99.974\n",
      "Epoch 051: | Loss: 0.00078 | Acc: 99.981\n",
      "Epoch 052: | Loss: 0.00081 | Acc: 99.979\n",
      "Epoch 053: | Loss: 0.00090 | Acc: 99.974\n",
      "Epoch 054: | Loss: 0.00086 | Acc: 99.976\n",
      "Epoch 055: | Loss: 0.00094 | Acc: 99.973\n",
      "Epoch 056: | Loss: 0.00082 | Acc: 99.976\n",
      "Epoch 057: | Loss: 0.00088 | Acc: 99.979\n",
      "Epoch 058: | Loss: 0.00075 | Acc: 99.978\n",
      "Epoch 059: | Loss: 0.00082 | Acc: 99.978\n",
      "Epoch 060: | Loss: 0.00078 | Acc: 99.976\n",
      "Epoch 061: | Loss: 0.00073 | Acc: 99.980\n",
      "Epoch 062: | Loss: 0.00082 | Acc: 99.977\n",
      "Epoch 063: | Loss: 0.00062 | Acc: 99.983\n",
      "Epoch 064: | Loss: 0.00082 | Acc: 99.972\n",
      "Epoch 065: | Loss: 0.00076 | Acc: 99.980\n",
      "Epoch 066: | Loss: 0.00089 | Acc: 99.973\n",
      "Epoch 067: | Loss: 0.00074 | Acc: 99.973\n",
      "Epoch 068: | Loss: 0.00078 | Acc: 99.979\n",
      "Epoch 069: | Loss: 0.00076 | Acc: 99.976\n",
      "Epoch 070: | Loss: 0.00072 | Acc: 99.979\n",
      "Epoch 071: | Loss: 0.00065 | Acc: 99.981\n",
      "Epoch 072: | Loss: 0.00069 | Acc: 99.980\n",
      "Epoch 073: | Loss: 0.00069 | Acc: 99.983\n",
      "Epoch 074: | Loss: 0.00061 | Acc: 99.980\n",
      "Epoch 075: | Loss: 0.00068 | Acc: 99.982\n",
      "Epoch 076: | Loss: 0.00062 | Acc: 99.981\n",
      "Epoch 077: | Loss: 0.00063 | Acc: 99.982\n",
      "Epoch 078: | Loss: 0.00062 | Acc: 99.980\n",
      "Epoch 079: | Loss: 0.00083 | Acc: 99.979\n",
      "Epoch 080: | Loss: 0.00061 | Acc: 99.983\n",
      "Epoch 081: | Loss: 0.00067 | Acc: 99.981\n",
      "Epoch 082: | Loss: 0.00052 | Acc: 99.985\n",
      "Epoch 083: | Loss: 0.00059 | Acc: 99.981\n",
      "Epoch 084: | Loss: 0.00069 | Acc: 99.984\n",
      "Epoch 085: | Loss: 0.00081 | Acc: 99.977\n",
      "Epoch 086: | Loss: 0.00066 | Acc: 99.979\n",
      "Epoch 087: | Loss: 0.00073 | Acc: 99.981\n",
      "Epoch 088: | Loss: 0.00065 | Acc: 99.981\n",
      "Epoch 089: | Loss: 0.00068 | Acc: 99.984\n",
      "Epoch 090: | Loss: 0.00064 | Acc: 99.982\n",
      "Epoch 091: | Loss: 0.00064 | Acc: 99.981\n",
      "Epoch 092: | Loss: 0.00064 | Acc: 99.980\n",
      "Epoch 093: | Loss: 0.00053 | Acc: 99.986\n",
      "Epoch 094: | Loss: 0.00062 | Acc: 99.985\n",
      "Epoch 095: | Loss: 0.00068 | Acc: 99.980\n",
      "Epoch 096: | Loss: 0.00071 | Acc: 99.981\n",
      "Epoch 097: | Loss: 0.00064 | Acc: 99.983\n",
      "Epoch 098: | Loss: 0.00058 | Acc: 99.984\n",
      "Epoch 099: | Loss: 0.00060 | Acc: 99.981\n",
      "Epoch 100: | Loss: 0.00058 | Acc: 99.984\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        #X_batch, y_batch = X_batch.to(\"cpu\"), y_batch.to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1).float())\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "    \n",
    "\n",
    "torch.save(model.state_dict(), './credit_card_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "orange-vision",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-10e318238336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'labels'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "psychological-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        #print(X_batch)\n",
    "        y_test_pred = model(X_batch[0])\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "positive-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56875\n",
      "           1       0.93      0.62      0.74        87\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.81      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "intellectual-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56871,     4],\n",
       "       [   33,    54]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fifth-seating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
